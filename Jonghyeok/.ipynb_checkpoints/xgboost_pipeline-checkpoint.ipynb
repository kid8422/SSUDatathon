{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f8bd10-a8ee-49ad-9944-55ceac5af220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import optuna\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "# iris 데이터셋을 로드합니다.\n",
    "iris = load_iris()\n",
    "\n",
    "# 데이터셋을 분리합니다.\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# print(X.shape)\n",
    "# (150, 4)\n",
    "# print(y.shape)\n",
    "# (150,)\n",
    "\n",
    "\n",
    "# 데이터셋을 8:2 비율로 train과 test로 분할합니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_validation, X_test, y_validation, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    # 최적화할 XGBoost 파라미터입니다.\n",
    "    params = {\n",
    "        \"objective\": \"multi:softprob\",\n",
    "        \"eval_metric\": 'mlogloss',\n",
    "        \"booster\": 'gbtree',\n",
    "        \"tree_method\": 'hist',\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 4, 10),\n",
    "        \"learning_rate\": trial.suggest_float('learning_rate', 0.0001, 0.99),\n",
    "        'n_estimators': trial.suggest_int(\"n_estimators\", 1000, 10000, step=100),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.5, 1.0),\n",
    "        \"colsample_bynode\": trial.suggest_float(\"colsample_bynode\", 0.5, 1.0),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-2, 1.0),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-2, 1.0),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 2, 15),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0.1, 1.0),\n",
    "        \"random_state\": RANDOM_SEED,\n",
    "    }\n",
    "   \n",
    "    # 파이프 라인을 구성합니다.\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', XGBClassifier(**params))\n",
    "    ])\n",
    "   \n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = pipeline.predict(X_validation)\n",
    "\n",
    "    accuracy = accuracy_score(y_validation, y_pred)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Optuna를 사용한 하이퍼파라미터 최적화를 시작합니다.\n",
    "start_time = time.time()\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED))\n",
    "study.optimize(objective, n_trials=15)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f'실행시간 = {end_time - start_time:.2f} 초')\n",
    "print(f'Best accuracy: {study.best_value:.4f}')\n",
    "print('Best hyperparameters:', study.best_params)\n",
    "\n",
    "\n",
    "# 이제 Optuna를 사용하여 구한 최적의 하이퍼파라미터로 최종 파이프라인 학습을 진행합니다.\n",
    "best_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', XGBClassifier(random_state=RANDOM_SEED, **study.best_params))\n",
    "])\n",
    "\n",
    "best_pipeline.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# 파이프라인을 저장합니다.\n",
    "joblib.dump(best_pipeline, 'best_pipeline.pkl')\n",
    "\n",
    "print(\"파이프라인이 'best_pipeline.pkl'로 저장되었습니다.\")\n",
    "\n",
    "\n",
    "\n",
    "# 저장된 파이프라인을 로드합니다.\n",
    "loaded_pipeline = joblib.load('best_pipeline.pkl')\n",
    "print(\"'best_pipeline.pkl'에서 파이프 라인을 로드했습니다.\")\n",
    "\n",
    "\n",
    "# 테스트 데이터로 추론합니다.\n",
    "y_pred = loaded_pipeline.predict(X_test)\n",
    "\n",
    "\n",
    "# 정확도를 계산합니다.\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Test Accuracy: {accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datathon",
   "language": "python",
   "name": "datathon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
